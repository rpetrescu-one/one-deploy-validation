---
ansible_python_interpreter: /usr/bin/python3

ansible_user: root
ensure_keys_for: [root]

ensure_hostname: true
ensure_hosts: true

unattend_disable: true
update_pkg_cache: true

one_version: '6.10'
one_pass: '<oneadmin_password_here>'

# ansible key to use
ansible_ssh_private_key_file: '<ssh_key>'

validation:

  # TEST CASE NAME: Core Services
  # DESCRIPTION: Check the status of OpenNebula core services, as listed in `service_list`,
  # also tests the resilience of some of them by restarting. 
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_core_services: true
  core_services:
    service_list:
      - name: opennebula.service
        desc: OpenNebula core (oned)
      - name: opennebula-gate.service
        desc: OpenNebula gate
      - name: opennebula-flow.service
        desc: OpenNebula flow
      # NOTE: Scheduler service is always checked in earlier than 6.99 versions
      # - name: opennebula-scheduler.service
      #   desc: OpenNebula Scheduler
      - name: opennebula-fireedge.service
        desc: OpenNebula Fireedge GUI
      # Add other services if more should be checked.
    check_fireedge_ui: true

  # TEST CASE NAME: Storage Benchmark
  # DESCRIPTION: Instantiates a VM for running storage benchmark from it. The VM needs to
  # have public internet access to install dependencies. The VM's connected network can
  # be configured.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_storage_benchmark: true
  storage_benchmark:
    vnet_name: evpn0

  # TEST CASE NAME: Network Benchmark
  # DESCRIPTION: Runs an iperf bandwidth and ping benchmark between all hypervisor hosts
  # that are listed in the inventory.
  # OUTPUT: Ansible execution output in the command line
  run_network_benchmark: true
  network_benchmark:
    iperf_port: 5201
    iperf_test_time: 10

  # TEST CASE NAME: Connectivity Matrix
  # DESCRIPTION: Instantiates a test VM on each hypervisor hosts registered in the 
  # OpenNebula cloud. From each host accesses the VM (temporarily sets up local routing
  # on the configured bridge) and runs a connectivity test to all other VMs running on
  # the other hosts.
  # OUTPUT: 
  #    - HTML document at /tmp/conn-matrix-report.html
  #    - Raw JSON data at /tmp/conn-matrix-raw-data.json
  run_conn_matrix: true
  conn_matrix:
    bridge_name: onebr0
    ping_count: 10
    vnet_name: evpn0
    market_name: 'Alpine Linux 3.21'

  # TEST CASE NAME: VM instantiation
  # DESCRIPTION: Fetches a VM template and image from the OpenNebula Marketplace and instantiates it.
  # Optionally also instantiates a VNET, attaches it to the VM, and checks connectivity.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_test_vm: true
  test_vm:
    vm:
      check_connection: true
      market_name: 'Alpine Linux 3.21'
      template_extra: |
        MEMORY="512"
        CONTEXT=[
          NETWORK="YES",
          REPORT_READY="YES",
          SSH_PUBLIC_KEY="$USER[SSH_PUBLIC_KEY]",
          TOKEN="YES" 
        ]
    create_vnet: false
    vnet:
      name: 'evpn0'
      desc: 'A test network for post-deployment cloud verification'
      bridge: 'br-test'
      vn_mad: 'dummy'
      phydev: 'eth1'
      network_address: '192.168.150.100'
      network_mask: '255.255.255.0'
      dns: '8.8.8.8'
      gateway: '192.168.150.1'
      ar:
        - type: "IP4"
          ip: '192.168.150.100'
          size: '10'

  # TEST CASE NAME: VM High Availability
  # DESCRIPTION: The prerequisite is to have VM HA configured, see details in OpenNebula documentation.
  # Instantiates a VM and produces an error on its host with a configurable method (e.g. bringing the 
  # main interface down). Checks that the error is detected and the VM is migrated to another host.
  # NOTE: Blocks the test execution and asks for manual confirmation before the fencing occurs (that will shut
  # down the host). Does not provide an automatic way to recover the host.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_vm_ha: false
  vm_ha:
    produce_error_method: 'if_down'
    if_down_params:
      interface_name: 'br-test'
    fencing_check_retries: 8
    fencing_check_delay: 60
    vm_market_name: 'Alpine Linux 3.21'

  # TEST CASE NAME: Front-end High Availability
  # DESCRIPTION: The prerequisite is to have the Front-end HA configured, see details in OpenNebula documentation.
  # Checks that config file contents are the same on all FEs, checks the selected leader, simulates a leader failover
  # by stopping the opennebula service and verifies that a new leader is selected.
  # OUTPUT: HTML document at /tmp/fe_ha_report.html
  run_fe_ha: false
  fe_ha:
    one_config_path:
      - /etc/one
      - /var/lib/one/remotes/etc
    one_zone_name: OpenNebula

  # TEST CASE NAME: Federation Validation
  # DESCRIPTION: Validates OpenNebula Federation configuration and data synchronization across zones.
  # Checks federation configuration (MODE, ZONE_ID, MASTER_ONED), zone discovery, endpoint connectivity,
  # federated data synchronization (users, groups, VDCs, ACL rules, marketplaces), database backend consistency,
  # FireEdge configuration, and oneadmin user consistency across zones.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_federation: false
  federation:
    # Minimum number of zones expected in the federation
    min_zones: 2
    # Timeout for endpoint connectivity tests (seconds)
    endpoint_timeout: 10
    # Check FireEdge configuration
    check_fireedge: true
    # Check database backend consistency
    check_db_backend: true
    # Verify oneadmin user consistency
    verify_oneadmin: true
    # Zone group (defaults to frontend_group or 'frontend')
    zone_group: frontend
    # XML-RPC port (default: 2633)
    xmlrpc_port: 2633
    # gRPC port (default: 2634)
    grpc_port: 2634
    # Configuration files
    oned_conf: /etc/one/oned.conf
    fireedge_conf: /etc/one/fireedge-server.conf
