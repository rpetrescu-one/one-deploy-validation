---
ansible_python_interpreter: /usr/bin/python3

ansible_user: root
ensure_keys_for: [root]

ensure_hostname: true
ensure_hosts: true

unattend_disable: true
update_pkg_cache: true

one_version: '6.10'
one_pass: '<oneadmin_password_here>'

# ansible key to use
ansible_ssh_private_key_file: '<ssh_key>'

validation:

  # TEST CASE NAME: Core Services
  # DESCRIPTION: Check the status of OpenNebula core services, as listed in `service_list`,
  # also tests the resilience of some of them by restarting. 
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_core_services: true
  core_services:
    service_list:
      - name: opennebula.service
        desc: OpenNebula core (oned)
      - name: opennebula-gate.service
        desc: OpenNebula gate
      - name: opennebula-flow.service
        desc: OpenNebula flow
      # NOTE: Scheduler service is always checked in earlier than 6.99 versions
      # - name: opennebula-scheduler.service
      #   desc: OpenNebula Scheduler
      - name: opennebula-fireedge.service
        desc: OpenNebula Fireedge GUI
      # Add other services if more should be checked.
    check_fireedge_ui: true

  # TEST CASE NAME: Storage Benchmark
  # DESCRIPTION: Instantiates a VM for running storage benchmark from it. The VM needs to
  # have public internet access to install dependencies. The VM's connected network can
  # be configured.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_storage_benchmark: true
  storage_benchmark:
    vnet_name: evpn0

  # TEST CASE NAME: Network Benchmark
  # DESCRIPTION: Runs an iperf bandwidth and ping benchmark between all hypervisor hosts
  # that are listed in the inventory.
  # OUTPUT: Ansible execution output in the command line
  run_network_benchmark: true
  network_benchmark:
    iperf_port: 5201
    iperf_test_time: 10

  # TEST CASE NAME: Connectivity Matrix
  # DESCRIPTION: Instantiates a test VM on each hypervisor hosts registered in the 
  # OpenNebula cloud. From each host accesses the VM (temporarily sets up local routing
  # on the configured bridge) and runs a connectivity test to all other VMs running on
  # the other hosts.
  # OUTPUT: 
  #    - HTML document at /tmp/conn-matrix-report.html
  #    - Raw JSON data at /tmp/conn-matrix-raw-data.json
  run_conn_matrix: true
  conn_matrix:
    bridge_name: onebr0
    ping_count: 10
    vnet_name: evpn0
    market_name: 'Alpine Linux 3.21'

  # TEST CASE NAME: VM instantiation
  # DESCRIPTION: Fetches a VM template and image from the OpenNebula Marketplace and instantiates it.
  # Optionally also instantiates a VNET, attaches it to the VM, and checks connectivity.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_test_vm: true
  test_vm:
    vm:
      check_connection: true
      market_name: 'Alpine Linux 3.21'
      template_extra: |
        MEMORY="512"
        CONTEXT=[
          NETWORK="YES",
          REPORT_READY="YES",
          SSH_PUBLIC_KEY="$USER[SSH_PUBLIC_KEY]",
          TOKEN="YES" 
        ]
    create_vnet: false
    vnet:
      name: 'evpn0'
      desc: 'A test network for post-deployment cloud verification'
      bridge: 'br-test'
      vn_mad: 'dummy'
      phydev: 'eth1'
      network_address: '192.168.150.100'
      network_mask: '255.255.255.0'
      dns: '8.8.8.8'
      gateway: '192.168.150.1'
      ar:
        - type: "IP4"
          ip: '192.168.150.100'
          size: '10'

  # TEST CASE NAME: LDAP Authentication
  # DESCRIPTION: Validates LDAP bind, OpenNebula LDAP auth configuration, LDAP login to XML-RPC,
  # FireEdge UI login via LDAP, and verifies local auth fallback.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_ldap_auth: true
  ldap_auth:
    server: "172.20.0.7:389"
    base_dn: "dc=example,dc=com"
    bind_dn: "uid=admin,dc=example,dc=com"
    bind_pass: "newpassword"
    ldap_user: "oneuser"
    ldap_pass: "onepassword"
    local_user: "oneadmin"
    local_auth_file: "/var/lib/one/.one/one_auth"
    group_mapping:
      expected_group_name: "opennebula-group"
      expected_group_id: ""
      ldap_group_dn: "cn=opennebula-group,ou=group,dc=example,dc=com"
      ldap_group_member_attr: "memberUid"
    fireedge_proto: "http"
    fireedge_login_endpoint: "/fireedge/api/auth/"
    fireedge_skip_tls_verify: true
    rpc2_proto: "http"
    rpc2_path: "/RPC2"
    rpc2_skip_tls_verify: true

  # TEST CASE NAME: VM High Availability
  # DESCRIPTION: The prerequisite is to have VM HA configured, see details in OpenNebula documentation.
  # Instantiates a VM and produces an error on its host with a configurable method (e.g. bringing the 
  # main interface down). Checks that the error is detected and the VM is migrated to another host.
  # NOTE: Blocks the test execution and asks for manual confirmation before the fencing occurs (that will shut
  # down the host). Does not provide an automatic way to recover the host.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_vm_ha: false
  vm_ha:
    produce_error_method: 'if_down'
    if_down_params:
      interface_name: 'br-test'
    fencing_check_retries: 8
    fencing_check_delay: 60
    vm_market_name: 'Alpine Linux 3.21'

  # TEST CASE NAME: Front-end High Availability
  # DESCRIPTION: The prerequisite is to have the Front-end HA configured, see details in OpenNebula documentation.
  # Checks that config file contents are the same on all FEs, checks the selected leader, simulates a leader failover
  # by stopping the opennebula service and verifies that a new leader is selected.
  # OUTPUT: HTML document at /tmp/fe_ha_report.html
  run_fe_ha: false
  fe_ha:
    one_config_path:
      - /etc/one
      - /var/lib/one/remotes/etc
    one_zone_name: OpenNebula
